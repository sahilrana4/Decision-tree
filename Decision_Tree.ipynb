{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "=> A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It models decisions based on input features by splitting data at each node according to feature values. In classification, each leaf node represents a class label, and branches denote feature conditions; the tree predicts class labels by traversing nodes based on feature values until it reaches a leaf.\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "=> Gini Impurity measures the likelihood of incorrectly classifying a randomly chosen element. Lower Gini Impurity indicates purer nodes.\n",
        "\n",
        "Entropy quantifies uncertainty or disorder; lower entropy means higher purity. Decision Trees use these measures to determine splits, choosing splits that result in the lowest impurity or entropy and thus the most homogeneous nodes.\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "=> Pre-Pruning: Stops tree growth early by setting constraints (e.g., max depth, min samples per leaf). Advantage: Prevents overfitting by controlling complexity during training.\n",
        "\n",
        "Post-Pruning: Grows a full tree, then removes unnecessary nodes. Advantage: Can yield more accurate models by optimizing after seeing all data.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "=> Information Gain measures the reduction in impurity after a dataset is split on a feature. The split with the highest gain is chosen, as it yields the purest child subsets. It's crucial for identifying the most informative features and creating effective splits.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "=> Applications: Medical diagnosis, customer segmentation, credit scoring, fraud detection.\n",
        "\n",
        "Advantages: Intuitive, easy to interpret, handle both numeric and categorical data well.\n",
        "\n",
        "Limitations: Prone to overfitting, sensitive to small changes in data, less effective with highly correlated features.\n",
        "\n"
      ],
      "metadata": {
        "id": "jSnaWeB4xMWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Python Program (Iris, Gini criterion, Accuracy & Feature Importances)"
      ],
      "metadata": {
        "id": "7aPr0WafxcK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X, y)\n",
        "pred = clf.predict(X)\n",
        "print('Accuracy:', accuracy_score(y, pred))\n",
        "print('Feature Importances:', clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAPqHyoTxzYY",
        "outputId": "a8cb075a-c8c5-4381-eeb5-d25509f95e7d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.02666667 0.         0.05072262 0.92261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Python Program (Iris, max_depth=3 vs fully-grown tree accuracy)"
      ],
      "metadata": {
        "id": "SB_4C2s2x35P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_full.fit(X, y)\n",
        "accuracy_full = accuracy_score(y, clf_full.predict(X))\n",
        "\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "clf_depth3.fit(X, y)\n",
        "accuracy_depth3 = accuracy_score(y, clf_depth3.predict(X))\n",
        "\n",
        "print('Fully-grown tree accuracy:', accuracy_full)\n",
        "print('max_depth=3 tree accuracy:', accuracy_depth3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB5f3TKbx2Ln",
        "outputId": "b4c297be-d24b-4559-e1e9-30e9e5fd4c46"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown tree accuracy: 1.0\n",
            "max_depth=3 tree accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Python Program (Boston Housing, Decision Tree Regressor, MSE & Feature Importances)"
      ],
      "metadata": {
        "id": "wObTfHU2yY_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load_boston dataset has been removed from scikit-learn since v1.2 due to ethical concerns.\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X, y)\n",
        "pred = reg.predict(X)\n",
        "print('Mean Squared Error:', mean_squared_error(y, pred))\n",
        "print('Feature Importances:', reg.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE-tmBo6yeCd",
        "outputId": "12365b53-2c2b-41f1-9ae3-c0df420f99e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 9.35625724797014e-32\n",
            "Feature Importances: [0.52562746 0.0509582  0.05302283 0.02771999 0.0317255  0.13125989\n",
            " 0.09416251 0.08552361]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Python Program (Iris, GridSearchCV for max_depth, min_samples_split)"
      ],
      "metadata": {
        "id": "fC7WbPbBza6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # y should be 0/1/2\n",
        "\n",
        "params = {'max_depth': [2,3,4,5], 'min_samples_split': [2,3,4]}\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), params, cv=3)\n",
        "grid.fit(X, y)\n",
        "print('Best Parameters:', grid.best_params_)\n",
        "print('Best Accuracy:', grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c4M0vw_ygMk",
        "outputId": "74be8266-1194-4dfc-de93-0a8b27eb507d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 3}\n",
            "Best Accuracy: 0.9733333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Healthcare Prediction - Data Science Process?\n",
        "\n",
        "=> Handle missing values: Impute using mean/median for numeric, mode or a placeholder for categorical.\n",
        "\n",
        "Encode categorical features: Use label encoding or one-hot encoding as appropriate.\n",
        "\n",
        "Train a Decision Tree model: Split data into train/test sets, fit classifier.\n",
        "\n",
        "Tune hyperparameters: Use GridSearchCV to optimize parameters like max_depth, min_samples_split.\n",
        "\n",
        "Evaluate performance: Assess using metrics like accuracy, precision, recall, F1-score.\n",
        "\n",
        "Business value: Enables data-driven, consistent disease prediction, improves efficiency, reduces manual analysis, and can help prioritize patient care and resources."
      ],
      "metadata": {
        "id": "LCg56gWD0dpq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xGgoXFBEzi15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}